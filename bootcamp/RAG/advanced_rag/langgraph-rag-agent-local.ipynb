{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f53f753-12c6-4fac-b910-6e96677d8a49",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/langgraph-rag-agent-local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ab14a-fd80-4ca2-afc5-efe1c39532bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-huggingface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0216de30-29cf-4464-9cc3-6e9a6d6c3e40",
   "metadata": {},
   "source": [
    "# Local LangGraph RAG agent with Llama 3\n",
    "\n",
    "\n",
    "Let's build an Advanced RAG that will run everything locally, for that we will use Ollama.\n",
    "\n",
    "## Ideas\n",
    "\n",
    "We'll combine ideas from three RAG papers into a RAG agent:\n",
    "\n",
    "- **Routing:**  Adaptive RAG ([paper](https://arxiv.org/abs/2403.14403)). Route questions to different retrieval approaches\n",
    "- **Fallback:** Corrective RAG ([paper](https://arxiv.org/pdf/2401.15884.pdf)). Fallback to web search if docs are not relevant to query\n",
    "- **Self-correction:** Self-RAG ([paper](https://arxiv.org/abs/2310.11511)). Fix answers w/ hallucinations or donâ€™t address question\n",
    "\n",
    "![langgraph_adaptive_rag.png](imgs/RAG_Agent_langGraph.png)\n",
    "\n",
    "Note that this will incorperate [a few general ideas for agents](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/):\n",
    "\n",
    "- **Reflection**: The self-correction mechanism is a form of reflection, where the LangGraph agent reflects on its retrieval and generations\n",
    "- **Planning**: The control flow laid out in the graph is a form of planning \n",
    "- **Tool use**: Specific nodes in the control flow (e.g., web search) will use tools\n",
    "\n",
    "## Local models\n",
    "\n",
    "### LLM\n",
    "\n",
    "Use [Ollama](https://ollama.ai/) and [llama3](https://ollama.ai/library/llama3):\n",
    "\n",
    "```\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "### Search\n",
    "\n",
    "Uses [Tavily](https://tavily.com/#api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2242c5-0dbb-4f21-9771-92f6d679b1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3897d23e-011f-469c-ad72-829c429e2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2096d49c-d3dc-4329-ada7-aff56d210198",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "\n",
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c63e1-4c2f-439d-8d95-4c6aa01f41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus\",\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    connection_args={\"uri\": \"./milvus_rag.db\"},\n",
    "\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"document\": \"Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"document\": \"Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n    \\n    Here is the user question: \\n    agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [13.79s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:10.275202Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 13788816541,\n",
      "          \"load_duration\": 12087986166,\n",
      "          \"prompt_eval_count\": 375,\n",
      "          \"prompt_eval_duration\": 1475608000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 215448000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:10.275202Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 13788816541,\n",
      "              \"load_duration\": 12087986166,\n",
      "              \"prompt_eval_count\": 375,\n",
      "              \"prompt_eval_duration\": 1475608000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 215448000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0c061207-eb70-4542-86c2-972d82c24afa-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [13.80s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: agent memory \\n    Context: [Document(page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287843}), Document(page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\\\n\\\\n\\\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\\\n\\\\n\\\\nCitation#\\\\nCited as:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287865}), Document(page_content=\\\"LLM Powered Autonomous Agents | Lil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nPosts\\\\n\\\\n\\\\n\\\\n\\\\nArchive\\\\n\\\\n\\\\n\\\\n\\\\nSearch\\\\n\\\\n\\\\n\\\\n\\\\nTags\\\\n\\\\n\\\\n\\\\n\\\\nFAQ\\\\n\\\\n\\\\n\\\\n\\\\nemojisearch.app\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n      LLM Powered Autonomous Agents\\\\n    \\\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\\\n\\\\n\\\\n \\\\n\\\\n\\\\nTable of Contents\\\\n\\\\n\\\\n\\\\nAgent System Overview\\\\n\\\\nComponent One: Planning\\\\n\\\\nTask Decomposition\\\\n\\\\nSelf-Reflection\\\\n\\\\n\\\\nComponent Two: Memory\\\\n\\\\nTypes of Memory\\\\n\\\\nMaximum Inner Product Search (MIPS)\\\\n\\\\n\\\\nComponent Three: Tool Use\\\\n\\\\nCase Studies\\\\n\\\\nScientific Discovery Agent\\\\n\\\\nGenerative Agents Simulation\\\\n\\\\nProof-of-Concept Examples\\\\n\\\\n\\\\nChallenges\\\\n\\\\nCitation\\\\n\\\\nReferences\\\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287808}), Document(page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\\\nGenerative Agents Simulation#\\\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\\\n\\\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agentsâ€™ experience in natural language.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287842})] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [9.77s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:20.56843Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 9760636209,\n",
      "          \"load_duration\": 7185750,\n",
      "          \"prompt_eval_count\": 1924,\n",
      "          \"prompt_eval_duration\": 7638702000,\n",
      "          \"eval_count\": 50,\n",
      "          \"eval_duration\": 2080698000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:20.56843Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 9760636209,\n",
      "              \"load_duration\": 7185750,\n",
      "              \"prompt_eval_count\": 1924,\n",
      "              \"prompt_eval_duration\": 7638702000,\n",
      "              \"eval_count\": 50,\n",
      "              \"eval_duration\": 2080698000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6c355015-2ea6-4b12-9ae8-07aafdb941e5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [9.77s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\"\n",
      "}\n",
      "The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0261a9a4-de13-4dd8-b082-95305a3e43ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287843}), Document(page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\\\n\\\\n\\\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\\\n\\\\n\\\\nCitation#\\\\nCited as:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287865}), Document(page_content=\\\"LLM Powered Autonomous Agents | Lil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nPosts\\\\n\\\\n\\\\n\\\\n\\\\nArchive\\\\n\\\\n\\\\n\\\\n\\\\nSearch\\\\n\\\\n\\\\n\\\\n\\\\nTags\\\\n\\\\n\\\\n\\\\n\\\\nFAQ\\\\n\\\\n\\\\n\\\\n\\\\nemojisearch.app\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n      LLM Powered Autonomous Agents\\\\n    \\\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\\\n\\\\n\\\\n \\\\n\\\\n\\\\nTable of Contents\\\\n\\\\n\\\\n\\\\nAgent System Overview\\\\n\\\\nComponent One: Planning\\\\n\\\\nTask Decomposition\\\\n\\\\nSelf-Reflection\\\\n\\\\n\\\\nComponent Two: Memory\\\\n\\\\nTypes of Memory\\\\n\\\\nMaximum Inner Product Search (MIPS)\\\\n\\\\n\\\\nComponent Three: Tool Use\\\\n\\\\nCase Studies\\\\n\\\\nScientific Discovery Agent\\\\n\\\\nGenerative Agents Simulation\\\\n\\\\nProof-of-Concept Examples\\\\n\\\\n\\\\nChallenges\\\\n\\\\nCitation\\\\n\\\\nReferences\\\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287808}), Document(page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\\\nGenerative Agents Simulation#\\\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\\\n\\\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agentsâ€™ experience in natural language.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287842})] \\n\\n    Here is the answer: \\n    The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [8.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:28.753962Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 8157651542,\n",
      "          \"load_duration\": 1651583,\n",
      "          \"prompt_eval_count\": 2003,\n",
      "          \"prompt_eval_duration\": 7890651000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 252381000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:28.753962Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 8157651542,\n",
      "              \"load_duration\": 1651583,\n",
      "              \"prompt_eval_count\": 2003,\n",
      "              \"prompt_eval_duration\": 7890651000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 252381000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b883b2d0-2c91-4aa7-8c26-3263574bafe4-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [8.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    {documents} \n",
    "\n",
    "    Here is the answer: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9f6944-4fee-4971-b3a7-2b81b44ed433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"generation\": \"The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"generation\": \"The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    The context discusses an agent's memory mechanism, which surfaces context to inform the agent's behavior based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent's future behavior. \\n\\n    Here is the question: agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [807ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:29.583869Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 799598625,\n",
      "          \"load_duration\": 1613750,\n",
      "          \"prompt_eval_count\": 137,\n",
      "          \"prompt_eval_duration\": 599784000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 195397000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:29.583869Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 799598625,\n",
      "              \"load_duration\": 1613750,\n",
      "              \"prompt_eval_count\": 137,\n",
      "              \"prompt_eval_duration\": 599784000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 195397000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c46afcb1-5ee8-4ca3-810e-e695740ea2c3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [816ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     \n",
    "    Here is the answer:\n",
    "    {generation} \n",
    "\n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c910c1-738c-4bf7-bf9e-801862b227eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Library/Caches/pypoetry/virtualenvs/milvus-bootcamp-rag-MiiP0ihC-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    llm agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [961ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n\\\"datasource\\\": \\\"vectorstore\\\"\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:30.888294Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 954609458,\n",
      "          \"load_duration\": 2615750,\n",
      "          \"prompt_eval_count\": 129,\n",
      "          \"prompt_eval_duration\": 614089000,\n",
      "          \"eval_count\": 11,\n",
      "          \"eval_duration\": 329666000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n\\\"datasource\\\": \\\"vectorstore\\\"\\n}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:30.888294Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 954609458,\n",
      "              \"load_duration\": 2615750,\n",
      "              \"prompt_eval_count\": 129,\n",
      "              \"prompt_eval_duration\": 614089000,\n",
      "              \"eval_count\": 11,\n",
      "              \"eval_duration\": 329666000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-382b4684-1af3-4e67-aad2-9e51fc5167b3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [963ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \n",
    "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explaination. \n",
    "    \n",
    "    Question to route: \n",
    "    {question}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd59cdf-a04d-4b2e-b9cc-6a1b1e80a6c6",
   "metadata": {},
   "source": [
    "We'll implement these as a control flow in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})  \n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f21594-00d4-48a8-ae2e-4e55a010b540",
   "metadata": {},
   "source": [
    "### Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13043b0f-17c7-49d3-9ea7-8f2c0f0c8691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "What are the types of agent memory?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [567ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:31.586775Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 564153000,\n",
      "          \"load_duration\": 1415792,\n",
      "          \"prompt_eval_count\": 13,\n",
      "          \"prompt_eval_duration\": 290251000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 265855000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:31.586775Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 564153000,\n",
      "              \"load_duration\": 1415792,\n",
      "              \"prompt_eval_count\": 13,\n",
      "              \"prompt_eval_duration\": 290251000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 265855000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e03528fc-308e-41ff-a6cb-4ec17a2b1ded-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [572ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO RAG---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [572ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"vectorstore\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<branch:__start__:route_question:retrieve>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<branch:__start__:route_question:retrieve>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [574ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"generation\": null,\n",
      "  \"web_search\": null,\n",
      "  \"documents\": null\n",
      "}\n",
      "---RETRIEVE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve > chain:ChannelWrite<retrieve,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve > chain:ChannelWrite<retrieve,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:retrieve] [287ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: retrieve:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n    \\n    Here is the user question: \\n    What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [1.56s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:33.445291Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1557596667,\n",
      "          \"load_duration\": 1924125,\n",
      "          \"prompt_eval_count\": 341,\n",
      "          \"prompt_eval_duration\": 1338650000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 202906000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:33.445291Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1557596667,\n",
      "              \"load_duration\": 1924125,\n",
      "              \"prompt_eval_count\": 341,\n",
      "              \"prompt_eval_duration\": 1338650000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 202906000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-25c23186-987b-4949-9ae6-d7f8ea8acc43-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [1.57s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n    \\n    Here is the user question: \\n    What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [834ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:34.287474Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 815232333,\n",
      "          \"load_duration\": 942875,\n",
      "          \"prompt_eval_count\": 145,\n",
      "          \"prompt_eval_duration\": 610609000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 200267000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:34.287474Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 815232333,\n",
      "              \"load_duration\": 942875,\n",
      "              \"prompt_eval_count\": 145,\n",
      "              \"prompt_eval_duration\": 610609000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 200267000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f5f6145b-0446-4521-9914-4a6f8942ea1e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [840ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. Iâ€™ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. Iâ€™ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. Iâ€™ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n    \\n    Here is the user question: \\n    What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [1.20s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:35.49943Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1196439000,\n",
      "          \"load_duration\": 874417,\n",
      "          \"prompt_eval_count\": 250,\n",
      "          \"prompt_eval_duration\": 989932000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 201826000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:35.49943Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1196439000,\n",
      "              \"load_duration\": 874417,\n",
      "              \"prompt_eval_count\": 250,\n",
      "              \"prompt_eval_duration\": 989932000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 201826000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-13328051-ed21-4d53-a7c6-b31a0bd09dec-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [1.21s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"document\": \"Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n    \\n    Here is the user question: \\n    What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > llm:ChatOllama] [1.18s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:36.691117Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1180158333,\n",
      "          \"load_duration\": 814125,\n",
      "          \"prompt_eval_count\": 242,\n",
      "          \"prompt_eval_duration\": 975929000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 199816000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:36.691117Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1180158333,\n",
      "              \"load_duration\": 814125,\n",
      "              \"prompt_eval_count\": 242,\n",
      "              \"prompt_eval_duration\": 975929000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 199816000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f9be696f-159a-46b1-8538-114e5815ad3e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:RunnableSequence] [1.19s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<grade_documents,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<grade_documents,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:decide_to_generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:decide_to_generate] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"generate\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<branch:grade_documents:decide_to_generate:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents > chain:ChannelWrite<branch:grade_documents:decide_to_generate:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:grade_documents] [4.82s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: grade_documents:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: What are the types of agent memory? \\n    Context: [Document(page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287843}), Document(page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287810}), Document(page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\\\nComponent Two: Memory#\\\\n(Big thank you to ChatGPT for helping me draft this section. Iâ€™ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\\\nTypes of Memory#\\\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\\\n\\\\n\\\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287823}), Document(page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\\\n\\\\n\\\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\\\n\\\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 8. Categorization of human memory.\\\\nWe can roughly consider the following mappings:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287824})] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [10.27s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:46.973776Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 10260897333,\n",
      "          \"load_duration\": 2571666,\n",
      "          \"prompt_eval_count\": 1635,\n",
      "          \"prompt_eval_duration\": 6446301000,\n",
      "          \"eval_count\": 96,\n",
      "          \"eval_duration\": 3801274000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:46.973776Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 10260897333,\n",
      "              \"load_duration\": 2571666,\n",
      "              \"prompt_eval_count\": 1635,\n",
      "              \"prompt_eval_duration\": 6446301000,\n",
      "              \"eval_count\": 96,\n",
      "              \"eval_duration\": 3801274000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-bfe2af11-df6d-471f-b146-450a68a60dc3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [10.28s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agentâ€™s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287843}), Document(page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287810}), Document(page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\\\nComponent Two: Memory#\\\\n(Big thank you to ChatGPT for helping me draft this section. Iâ€™ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\\\nTypes of Memory#\\\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\\\n\\\\n\\\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287823}), Document(page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\\\n\\\\n\\\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\\\n\\\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\\\n\\\\n\\\\n\\\\n\\\\nFig. 8. Categorization of human memory.\\\\nWe can roughly consider the following mappings:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \\\"LLM Powered Autonomous Agents | Lil'Log\\\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'pk': 450435941815287824})] \\n\\n    Here is the answer: \\n    According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [7.23s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:54.207967Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 7218229208,\n",
      "          \"load_duration\": 1624250,\n",
      "          \"prompt_eval_count\": 1762,\n",
      "          \"prompt_eval_duration\": 6960273000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 244688000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:54.207967Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 7218229208,\n",
      "              \"load_duration\": 1624250,\n",
      "              \"prompt_eval_count\": 1762,\n",
      "              \"prompt_eval_duration\": 6960273000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 244688000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-694516ff-f1ff-4f5c-a8d0-0e07f56c6a26-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [7.23s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"generation\": \"According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the types of agent memory?\",\n",
      "  \"generation\": \"According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural).\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    According to the provided context, there are two types of agent memory mentioned:\\n\\n1. Short-term memory: This is used for in-context learning and storing information temporarily.\\n2. Long-term memory: This provides the ability to retain and recall information over extended periods.\\n\\nThese types of memories are similar to those found in human brains, with short-term memory being equivalent to working memory and long-term memory having two subtypes: explicit (declarative) and implicit (procedural). \\n\\n    Here is the question: What are the types of agent memory?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [1.04s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:55.260804Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1035703083,\n",
      "          \"load_duration\": 1286250,\n",
      "          \"prompt_eval_count\": 197,\n",
      "          \"prompt_eval_duration\": 835545000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 196323000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:55.260804Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1035703083,\n",
      "              \"load_duration\": 1286250,\n",
      "              \"prompt_eval_count\": 197,\n",
      "              \"prompt_eval_duration\": 835545000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 196323000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-83c2268f-597b-4316-b36d-f1dc3c4f83c8-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [1.05s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [8.28s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"useful\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [18.56s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [24.25s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "('According to the provided context, there are two types of agent memory '\n",
      " 'mentioned:\\n'\n",
      " '\\n'\n",
      " '1. Short-term memory: This is used for in-context learning and storing '\n",
      " 'information temporarily.\\n'\n",
      " '2. Long-term memory: This provides the ability to retain and recall '\n",
      " 'information over extended periods.\\n'\n",
      " '\\n'\n",
      " 'These types of memories are similar to those found in human brains, with '\n",
      " 'short-term memory being equivalent to working memory and long-term memory '\n",
      " 'having two subtypes: explicit (declarative) and implicit (procedural).')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733ab80-7e7b-4b1a-9519-9242de647eda",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/8d449b67-6bc4-4ecf-9153-759cd21df24f/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbfcec3e-a09a-40b4-9c15-fead97bf4e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "Who are the Bears expected to draft first in the NFL draft?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    Who are the Bears expected to draft first in the NFL draft?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [870ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"web_search\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:06:56.164797Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 865271417,\n",
      "          \"load_duration\": 998542,\n",
      "          \"prompt_eval_count\": 138,\n",
      "          \"prompt_eval_duration\": 601310000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 260339000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"web_search\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:06:56.164797Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 865271417,\n",
      "              \"load_duration\": 998542,\n",
      "              \"prompt_eval_count\": 138,\n",
      "              \"prompt_eval_duration\": 601310000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 260339000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5b24a8c0-d47c-4edc-9415-6f32fd3a1045-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [875ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [876ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"websearch\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<branch:__start__:route_question:websearch>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<branch:__start__:route_question:websearch>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [877ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Who are the Bears expected to draft first in the NFL draft?\",\n",
      "  \"generation\": null,\n",
      "  \"web_search\": null,\n",
      "  \"documents\": null\n",
      "}\n",
      "---WEB SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] Entering Tool run with input:\n",
      "\u001b[0m\"{'query': 'Who are the Bears expected to draft first in the NFL draft?'}\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError(\"'list' object has no attribute 'strip'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] [1.79s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: websearch:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.87s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:01.829506Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3853036959,\n",
      "          \"load_duration\": 2831750,\n",
      "          \"prompt_eval_count\": 562,\n",
      "          \"prompt_eval_duration\": 2317009000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1518313000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:01.829506Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3853036959,\n",
      "              \"load_duration\": 2831750,\n",
      "              \"prompt_eval_count\": 562,\n",
      "              \"prompt_eval_duration\": 2317009000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1518313000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4ec65274-3632-4a4a-94b3-002eb4d69657-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.88s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.66s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:04.507696Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2651274166,\n",
      "          \"load_duration\": 884083,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2425181000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 218463000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:04.507696Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2651274166,\n",
      "              \"load_duration\": 884083,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2425181000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 218463000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-02a1a75c-90e8-4cdc-a932-0dad3349fc22-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.67s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.67s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.55s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:08.246145Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3723004833,\n",
      "          \"load_duration\": 894125,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2189420000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1528060000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:08.246145Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3723004833,\n",
      "              \"load_duration\": 894125,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2189420000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1528060000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4ee44c0d-299f-414b-8c82-4f2b0c62743e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.65s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:10.903591Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2641114292,\n",
      "          \"load_duration\": 881833,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2427621000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207766000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:10.903591Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2641114292,\n",
      "              \"load_duration\": 881833,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2427621000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207766000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ef0cdee6-4c9a-4c69-91b4-5ded92c7e16f-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.39s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:14.633219Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3715633833,\n",
      "          \"load_duration\": 871417,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2190063000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1520408000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:14.633219Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3715633833,\n",
      "              \"load_duration\": 871417,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2190063000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1520408000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c239b910-2ff6-489d-8950-21eeaa08e211-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:17.278194Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2629146042,\n",
      "          \"load_duration\": 900625,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2414923000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 208408000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:17.278194Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2629146042,\n",
      "              \"load_duration\": 900625,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2414923000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 208408000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-13ef1cfe-ec4b-4298-b29d-fdb783a33b51-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:21.020536Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3724077125,\n",
      "          \"load_duration\": 920958,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2195713000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1521239000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:21.020536Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3724077125,\n",
      "              \"load_duration\": 920958,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2195713000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1521239000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-103b588f-2d74-4dca-b0d9-406559bb3ed8-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.74s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:23.676628Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2638764959,\n",
      "          \"load_duration\": 906875,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2420263000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 212788000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:23.676628Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2638764959,\n",
      "              \"load_duration\": 906875,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2420263000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 212788000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-072bdb97-6222-4cf1-82ee-160c0b5a7f6b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.39s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:27.388505Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3699568167,\n",
      "          \"load_duration\": 830875,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2180652000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1514095000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:27.388505Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3699568167,\n",
      "              \"load_duration\": 830875,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2180652000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1514095000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c6c2ffce-9b8a-40f9-a62a-a0b04a63a5ce-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:30.041153Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2629321917,\n",
      "          \"load_duration\": 836042,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2413741000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 209923000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:30.041153Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2629321917,\n",
      "              \"load_duration\": 836042,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2413741000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 209923000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ef05aa49-f607-4647-9214-bab5814c5ddf-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:33.769492Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3713510959,\n",
      "          \"load_duration\": 1116042,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2187961000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1519905000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:33.769492Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3713510959,\n",
      "              \"load_duration\": 1116042,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2187961000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1519905000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-770e9928-a9fd-41b4-84c8-4f337298f34c-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.65s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:36.423836Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2637715417,\n",
      "          \"load_duration\": 871375,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2424633000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207728000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:36.423836Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2637715417,\n",
      "              \"load_duration\": 871375,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2424633000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207728000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3dd4e5ab-d45a-4483-b3c2-0b0a35f50080-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.66s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.38s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:40.168809Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3720429250,\n",
      "          \"load_duration\": 1403083,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2188823000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1523738000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:40.168809Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3720429250,\n",
      "              \"load_duration\": 1403083,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2188823000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1523738000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-172da892-1232-40a0-b132-1935f887fb1b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.74s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:42.822763Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2629545125,\n",
      "          \"load_duration\": 2211708,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2415642000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 206694000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:42.822763Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2629545125,\n",
      "              \"load_duration\": 2211708,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2415642000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 206694000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-68913122-29e2-4b2f-bf0a-c6006096d5d4-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.39s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:46.537603Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3692452292,\n",
      "          \"load_duration\": 1516417,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2176450000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1509385000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:46.537603Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3692452292,\n",
      "              \"load_duration\": 1516417,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2176450000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1509385000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-171108e7-8340-4f3d-b401-5e48d60145cf-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:49.17608Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2621724000,\n",
      "          \"load_duration\": 917125,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2408980000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207331000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:49.17608Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2621724000,\n",
      "              \"load_duration\": 917125,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2408980000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207331000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9a88320b-871b-4739-8903-3db040246b16-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:52.889113Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3694330250,\n",
      "          \"load_duration\": 1070208,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2175740000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1512738000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:52.889113Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3694330250,\n",
      "              \"load_duration\": 1070208,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2175740000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1512738000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-28b26d33-b556-45b6-97d6-e4dfebb92786-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:55.519622Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2625620292,\n",
      "          \"load_duration\": 938709,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2408167000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 211392000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:55.519622Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2625620292,\n",
      "              \"load_duration\": 938709,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2408167000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 211392000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-cbcb61b0-67d1-42c9-b127-61caa0bbd77b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.63s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.34s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:07:59.23357Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3696920792,\n",
      "          \"load_duration\": 809375,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2182804000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1509256000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:07:59.23357Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3696920792,\n",
      "              \"load_duration\": 809375,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2182804000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1509256000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-afc7f0cf-839b-47d4-86c0-6d4bf2bc3e09-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:01.881143Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2624979750,\n",
      "          \"load_duration\": 1399042,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2409597000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 206964000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:01.881143Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2624979750,\n",
      "              \"load_duration\": 1399042,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2409597000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 206964000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-dd220d78-a466-4c33-bed5-a1eb207d8a45-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:05.604435Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3692143834,\n",
      "          \"load_duration\": 1456750,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2176545000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1507532000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:05.604435Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3692143834,\n",
      "              \"load_duration\": 1456750,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2176545000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1507532000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-9b8bdba9-77eb-47ad-a2b2-e5911505cb32-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:08.253142Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2627471625,\n",
      "          \"load_duration\": 1390875,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2412363000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 208357000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:08.253142Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2627471625,\n",
      "              \"load_duration\": 1390875,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2412363000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 208357000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-56f6bb6f-16b5-4dc5-8f80-e85715e0cf4d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:11.973307Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3697019792,\n",
      "          \"load_duration\": 1292292,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2177340000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1512290000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:11.973307Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3697019792,\n",
      "              \"load_duration\": 1292292,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2177340000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1512290000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a9f174e5-099f-4f6c-81b2-75ff8294f236-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:14.623534Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2626295834,\n",
      "          \"load_duration\": 1475709,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2410678000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207849000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:14.623534Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2626295834,\n",
      "              \"load_duration\": 1475709,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2410678000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207849000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a3000002-4984-45e5-bed3-bd5e3a939740-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.69s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:18.330784Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3688174041,\n",
      "          \"load_duration\": 1032666,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2175353000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1506773000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:18.330784Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3688174041,\n",
      "              \"load_duration\": 1032666,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2175353000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1506773000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-19f78c85-3c84-4a41-a94c-b6596086fc50-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:20.967128Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2623119833,\n",
      "          \"load_duration\": 802667,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2409021000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 208999000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:20.967128Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2623119833,\n",
      "              \"load_duration\": 802667,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2409021000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 208999000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b4702299-e383-4560-9f81-053e15062022-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.34s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:24.674165Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3687037916,\n",
      "          \"load_duration\": 1368125,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2177355000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1502925000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:24.674165Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3687037916,\n",
      "              \"load_duration\": 1368125,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2177355000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1502925000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f87b3962-2cb7-41a8-ac85-a6ea4fea315d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:27.330086Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2629917125,\n",
      "          \"load_duration\": 1466875,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2414938000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207713000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:27.330086Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2629917125,\n",
      "              \"load_duration\": 1466875,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2414938000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207713000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ec6fe4b9-6cb1-490a-8920-5b3cd1cc14cc-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:31.036746Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3694046833,\n",
      "          \"load_duration\": 950916,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2182175000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1505497000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:31.036746Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3694046833,\n",
      "              \"load_duration\": 950916,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2182175000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1505497000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-48d5cd08-fad2-449c-8a69-ba5690875811-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:33.681164Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2623548584,\n",
      "          \"load_duration\": 1372375,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2411505000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 205373000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:33.681164Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2623548584,\n",
      "              \"load_duration\": 1372375,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2411505000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 205373000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-98dba185-91d1-4acc-8fb7-97ac6e09fc76-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:37.387111Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3688054833,\n",
      "          \"load_duration\": 906791,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2175030000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1508045000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:37.387111Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3688054833,\n",
      "              \"load_duration\": 906791,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2175030000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1508045000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7530ddd4-fa59-42e1-b7d4-4d1bc2a48f07-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:40.036592Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2624755709,\n",
      "          \"load_duration\": 928250,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2409473000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 209129000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:40.036592Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2624755709,\n",
      "              \"load_duration\": 928250,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2409473000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 209129000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5cb176e2-e3d3-4a80-8dec-2c3257b5346d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:43.757246Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3703002667,\n",
      "          \"load_duration\": 803208,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2177310000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1520686000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:43.757246Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3703002667,\n",
      "              \"load_duration\": 803208,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2177310000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1520686000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-fba7831d-f9ca-41ea-a1a5-4c78891960a0-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:46.401103Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2620938333,\n",
      "          \"load_duration\": 827250,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2407693000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207682000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:46.401103Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2620938333,\n",
      "              \"load_duration\": 827250,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2407693000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207682000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-372b1ff9-54b3-4211-9817-1730aa2181ca-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:50.114217Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3691137042,\n",
      "          \"load_duration\": 1534125,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2182583000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1501634000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:50.114217Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3691137042,\n",
      "              \"load_duration\": 1534125,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2182583000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1501634000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-72c5a4c7-7da7-40ec-b2e1-3a72707436cb-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:52.764036Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2625095042,\n",
      "          \"load_duration\": 1313875,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2409059000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 209029000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:52.764036Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2625095042,\n",
      "              \"load_duration\": 1313875,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2409059000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 209029000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4878d693-6438-490a-9f4a-9b30785f64a1-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [6ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:56.498899Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3702815833,\n",
      "          \"load_duration\": 1392167,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2186567000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1508621000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:56.498899Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3702815833,\n",
      "              \"load_duration\": 1392167,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2186567000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1508621000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6cc7659c-df89-49ea-8643-420db0b7bdc1-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:08:59.132465Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2620867042,\n",
      "          \"load_duration\": 766000,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2406957000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 208835000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:08:59.132465Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2620867042,\n",
      "              \"load_duration\": 766000,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2406957000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 208835000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e3e10d39-034d-477e-ba6d-e7d4dd48825e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:02.853224Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3694474458,\n",
      "          \"load_duration\": 1396875,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2178069000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1508948000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:02.853224Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3694474458,\n",
      "              \"load_duration\": 1396875,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2178069000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1508948000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5bccbbac-3eb5-44c6-ac7e-dbb1624e8a88-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:05.504162Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2625869416,\n",
      "          \"load_duration\": 1527833,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2408560000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 209310000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:05.504162Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2625869416,\n",
      "              \"load_duration\": 1527833,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2408560000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 209310000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b00618f3-74ec-4f60-ae21-5efc5828adc3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:09.210383Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3691485166,\n",
      "          \"load_duration\": 841500,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2182068000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1504519000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:09.210383Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3691485166,\n",
      "              \"load_duration\": 841500,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2182068000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1504519000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-73a9d2e5-7f4f-42bc-971d-746e83a0f319-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:11.847678Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2624864000,\n",
      "          \"load_duration\": 802250,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2411235000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 208418000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:11.847678Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2624864000,\n",
      "              \"load_duration\": 802250,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2411235000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 208418000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f4da4ba5-4207-4dc8-964b-819a802e62fa-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.34s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:15.556831Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3691073375,\n",
      "          \"load_duration\": 1015458,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2177585000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1507706000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:15.556831Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3691073375,\n",
      "              \"load_duration\": 1015458,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2177585000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1507706000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-eeef68db-d403-4553-8591-a7048772c55d-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:18.204032Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2624135834,\n",
      "          \"load_duration\": 1434709,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2409036000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 207581000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:18.204032Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2624135834,\n",
      "              \"load_duration\": 1434709,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2409036000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 207581000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8317bc1f-9f03-46af-9a80-3099608e6f57-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:21.923983Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3692493084,\n",
      "          \"load_duration\": 1419292,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2180558000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1503873000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:21.923983Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3692493084,\n",
      "              \"load_duration\": 1419292,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2180558000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1503873000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-94726fb7-b12d-4de4-8e30-60509009a01a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:24.575497Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2623800334,\n",
      "          \"load_duration\": 1349084,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2408410000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 208307000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:24.575497Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2623800334,\n",
      "              \"load_duration\": 1349084,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2408410000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 208307000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-520ec29a-c27c-4a86-ab1f-763bcee2ee11-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.37s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Who are the Bears expected to draft first in the NFL draft? \\n    Context: [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:28.295839Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3689759833,\n",
      "          \"load_duration\": 1936125,\n",
      "          \"prompt_eval_count\": 563,\n",
      "          \"prompt_eval_duration\": 2176987000,\n",
      "          \"eval_count\": 45,\n",
      "          \"eval_duration\": 1505154000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:28.295839Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3689759833,\n",
      "              \"load_duration\": 1936125,\n",
      "              \"prompt_eval_count\": 563,\n",
      "              \"prompt_eval_duration\": 2176987000,\n",
      "              \"eval_count\": 45,\n",
      "              \"eval_duration\": 1505154000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-76758db8-3324-4344-ae4c-5ed5122d3c2a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"The Chicago Bears made waves during the first round of the 2024 NFL draft, where they added two blue chip players to the roster with their top-10 selections â€” quarterback Caleb Williams (No. 1 ...\\\\n8:27 p.m.: Caleb Williams reacts energetically to Bears drafting Rome Odunze. Close to an hour after the Bears drafted Caleb Williams with the No. 1 pick in the 2024 NFL Draft, they remained at their No. 9 pick to take Washington wide receiver Rome Odunze. And the Bears' new quarterback offered an energetic response while watching backstage.\\\\nHere's a full list of the Bears picks- as things stand right now- for the 2024 NFL Draft: First round: No. 1 overall (via Carolina) First round: No. 9 overall\\\\nRound\\\\nPick\\\\n1\\\\n1\\\\n1\\\\n9\\\\n3\\\\n75\\\\n4\\\\n111\\\\n4\\\\n123\\\\n5\\\\n143\\\\nThe extra first-round pick comes by way of Carolina, who would have otherwise had the first overall selection.\\\\n Bears draft picks in 2025\\\\nThe only change in 2025 picks is the Chase Claypool trade, which sent Miami a seventh-rounder along with the wide receiver for a sixth-round pick from the Dolphins.\\\\n Bears draft picks in 2024, 2025 & Beyond: Full list\\\\nWelcome to the 2024 offseason; the offseason where Ryan Poles and the Chicago Bears own all of the power.\\\\n QB Kyler Murray (ARI)\\\\n2018: QB Baker Mayfield (CLE)\\\\n2017: QB Myles Garrett (CLE)\\\\n2016: QB Jared Goff (LAR)\\\\n2015: QB Jameis Winston (TB)\\\\n The Bears not only own the no. 1 overall pick in the 2024 NFL Draft, but also have the league's third-highest amount of cap space.\\\\n\\\\nThe Bears on Thursday, as expected, selected USC quarterback Caleb Williams with the No. 1 pick in the 2024 NFL Draft. Williams was widely considered the top prospect in a draft class loaded with talented quarterbacks.\\\")] \\n\\n    Here is the answer: \\n    The Chicago Bears are not expected to draft anyone first in the NFL draft because they already made their selection, drafting USC quarterback Caleb Williams with the No. 1 pick. They have no additional picks in the first round.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [2.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:09:30.943234Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2627929500,\n",
      "          \"load_duration\": 1316750,\n",
      "          \"prompt_eval_count\": 622,\n",
      "          \"prompt_eval_duration\": 2410888000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 209390000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"no\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:09:30.943234Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2627929500,\n",
      "              \"load_duration\": 1316750,\n",
      "              \"prompt_eval_count\": 622,\n",
      "              \"prompt_eval_duration\": 2410888000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 209390000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-878ed437-4282-4b39-8428-c5bf39313721-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"no\"\n",
      "}\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"not supported\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<branch:generate:grade_generation_v_documents_and_question:generate>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.36s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:LangGraph] [155.66s] Chain run errored with error:\n",
      "\u001b[0m\"GraphRecursionError('Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/stephen/Library/Caches/pypoetry/virtualenvs/milvus-bootcamp-rag-MiiP0ihC-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py\\\", line 1014, in stream\\n    raise GraphRecursionError(\\n\\n\\nlanggraph.errors.GraphRecursionError: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\"\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are the Bears expected to draft first in the NFL draft?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinished running: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/milvus-bootcamp-rag-MiiP0ihC-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1014\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout hitting a stop condition. You can increase the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit by setting the `recursion_limit` config key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(read_channels(channels, output_keys))\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key."
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9051bea-13fa-4eb3-b671-16f59755931d",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/c785f9c0-f519-4a38-ad5a-febb59a2139c/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac34852-3b21-477a-a576-45466722171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "Did Emmanuel Macron visit Germany recently?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    Did Emmanuel Macron visit Germany recently?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [2.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n\\\"datasource\\\": \\\"web_search\\\"\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:17:44.035356Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2698602583,\n",
      "          \"load_duration\": 1753438375,\n",
      "          \"prompt_eval_count\": 140,\n",
      "          \"prompt_eval_duration\": 612997000,\n",
      "          \"eval_count\": 11,\n",
      "          \"eval_duration\": 322335000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n\\\"datasource\\\": \\\"web_search\\\"\\n}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:17:44.035356Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 2698602583,\n",
      "              \"load_duration\": 1753438375,\n",
      "              \"prompt_eval_count\": 140,\n",
      "              \"prompt_eval_duration\": 612997000,\n",
      "              \"eval_count\": 11,\n",
      "              \"eval_duration\": 322335000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-cc7d7c06-91f6-47e2-9ac6-86420c64449e-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [2.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [2.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"websearch\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<branch:__start__:route_question:websearch>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<branch:__start__:route_question:websearch>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [2.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\",\n",
      "  \"generation\": null,\n",
      "  \"web_search\": null,\n",
      "  \"documents\": null\n",
      "}\n",
      "---WEB SEARCH---\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > tool:tavily_search_results_json] Entering Tool run with input:\n",
      "\u001b[0m\"{'query': 'Did Emmanuel Macron visit Germany recently?'}\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError(\"'list' object has no attribute 'strip'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch > chain:ChannelWrite<websearch,question,generation,web_search,documents>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] [1.44s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: websearch:'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---GENERATE---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: Did Emmanuel Macron visit Germany recently? \\n    Context: [Document(page_content=\\\"Updated 10:18 AM PDT, May 26, 2024. BERLIN (AP) â€” President Emmanuel Macron on Sunday started the first state visit to Germany by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the European Union's traditional leading powers ahead of European Parliament elections in which far-right parties in ...\\\\nPARIS/BERLIN - French President Emmanuel Macron arrived in Berlin on May 26 on the first state visit to Germany by a French president in a quarter of a century, seeking to ease recent tensions and ...\\\\nFor the first time in 24 years, a French president has paid a state visit to Germany. President Emmanuel Macron arrived on Sunday for a three-day trip, intended to emphasise the strong ties ...\\\\nEmmanuel Macron has begun the first state visit to Germany by a French president in 24 years, to boost ties between the two countries and to emphasise the importance of defending democracy against ...\\\\nFrench President Emmanuel Macron (L) and German President Frank-Walter Steinmeier address a joint press conference after talks at Bellevue presidential palace in Berlin, Germany on May 26, 2024.\\\")] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > llm:ChatOllama] [3.60s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:17:49.104292Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3591510792,\n",
      "          \"load_duration\": 1414208,\n",
      "          \"prompt_eval_count\": 341,\n",
      "          \"prompt_eval_duration\": 1450070000,\n",
      "          \"eval_count\": 65,\n",
      "          \"eval_duration\": 2129331000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:17:49.104292Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3591510792,\n",
      "              \"load_duration\": 1414208,\n",
      "              \"prompt_eval_count\": 341,\n",
      "              \"prompt_eval_duration\": 1450070000,\n",
      "              \"eval_count\": 65,\n",
      "              \"eval_duration\": 2129331000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6762099e-697f-49cb-818a-ab4b961f7aea-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:RunnableSequence] [3.61s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:ChannelWrite<generate,question,generation,web_search,documents>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "---CHECK HALLUCINATIONS---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(page_content=\\\"Updated 10:18 AM PDT, May 26, 2024. BERLIN (AP) â€” President Emmanuel Macron on Sunday started the first state visit to Germany by a French head of state in 24 years, a three-day trip meant to underline the strong ties between the European Union's traditional leading powers ahead of European Parliament elections in which far-right parties in ...\\\\nPARIS/BERLIN - French President Emmanuel Macron arrived in Berlin on May 26 on the first state visit to Germany by a French president in a quarter of a century, seeking to ease recent tensions and ...\\\\nFor the first time in 24 years, a French president has paid a state visit to Germany. President Emmanuel Macron arrived on Sunday for a three-day trip, intended to emphasise the strong ties ...\\\\nEmmanuel Macron has begun the first state visit to Germany by a French president in 24 years, to boost ties between the two countries and to emphasise the importance of defending democracy against ...\\\\nFrench President Emmanuel Macron (L) and German President Frank-Walter Steinmeier address a joint press conference after talks at Bellevue presidential palace in Berlin, Germany on May 26, 2024.\\\")] \\n\\n    Here is the answer: \\n    Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [1.89s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:17:51.009948Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1882450667,\n",
      "          \"load_duration\": 1505834,\n",
      "          \"prompt_eval_count\": 427,\n",
      "          \"prompt_eval_duration\": 1673540000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 201677000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:17:51.009948Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1882450667,\n",
      "              \"load_duration\": 1505834,\n",
      "              \"prompt_eval_count\": 427,\n",
      "              \"prompt_eval_duration\": 1673540000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 201677000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0f4d404b-6d8c-4531-bf00-0bb172279c80-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [1.90s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\",\n",
      "  \"generation\": \"Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\",\n",
      "  \"generation\": \"Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    Yes, Emmanuel Macron visited Germany recently. According to the context, he started the first state visit to Germany by a French head of state in 24 years on Sunday, which was intended to underline the strong ties between the two countries. The visit took place from May 26 to May 29, 2024. \\n\\n    Here is the question: Did Emmanuel Macron visit Germany recently?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > llm:ChatOllama] [819ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-06-13T11:17:51.833911Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 808161917,\n",
      "          \"load_duration\": 855500,\n",
      "          \"prompt_eval_count\": 153,\n",
      "          \"prompt_eval_duration\": 606988000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 195766000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-06-13T11:17:51.833911Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 808161917,\n",
      "              \"load_duration\": 855500,\n",
      "              \"prompt_eval_count\": 153,\n",
      "              \"prompt_eval_duration\": 606988000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 195766000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5daab8ae-62ba-4611-896d-5b7844ca1786-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question > chain:RunnableSequence] [824ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate > chain:grade_generation_v_documents_and_question] [2.73s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"useful\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:generate] [6.34s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "'Finished running: generate:'\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [10.52s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "('Yes, Emmanuel Macron visited Germany recently. According to the context, he '\n",
      " 'started the first state visit to Germany by a French head of state in 24 '\n",
      " 'years on Sunday, which was intended to underline the strong ties between the '\n",
      " 'two countries. The visit took place from May 26 to May 29, 2024.')\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Did Emmanuel Macron visit Germany recently?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
